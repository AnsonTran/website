# Files
A file is a named collection of data with some attributes:
* name
* owner
* location
* size
* protection
* creation time
* time of last access

File types can be encoded in its name or contents:
* Windows encodes type in name (.exe, .doc, .jpg, etc.)
* Unix encodes type in contents
	* magic numbers
	* initial characters (#! in shell scripts)

Some file operations provided by a Unix OS (C library):
* create(name)
* write(fd, buf, len)
* read(fd, buf, len)
* seek(fd, pos) - repositioning within file
* unlink(name) - delete
* truncate(fd, length)
* open(name, mode)
* close(fd)

Most modern OS file systems provide two simple file access methods:

**Sequential Access** - read bytes one at a time, in order
* read next
* write next

**Direct Access** - random access given block/byte number
* read n (byte at offset n)
* write n

Database systems support more sophisticated methods
* Record access
* Indexed access

![database-access](database-access.png)

## Open Files
Each process maintains an open file table, which maps the open files that the process is read/writing from to a file descriptor (index in the table). It also maintains the current position within the file that the process is using.

![process-file-table](process-file-table.png)

The OS also maintains a system-wide open file table. Each entry in the per-process table maps to an entry in the system-wide open-file table.

![system-file-table](system-file-table.png)

## File Buffer Cache
Applications reading and writing files are very local (not a lot of jumping back and forth). So, we can take advantage of this locality with caching.

We use a system-wide cache, used and shared by all processes. This allows the disk to perform as fast as memory. 

In the time of hard-disks, writing was a much more expensive operation than reading. There are different write caching strategies:
* Write-behind
	* Maintain a queue of uncommitted blocks
	* At regular intervals, flush the queue and perform the write operations
	* Unreliable, as queue is stored in volatile cache
* Battery backed-up RAM (NVRAM)
	* Same as write-behind, but we maintain the queue in RAM backed by a battery. 
	* NVRAM has to be very large if there is a lot of writes in the queue. So, it can get expensive
* Log-structured file system
	* We write data contiguously, at end of previous write

With read caching, the most commonly implemented method is "read ahead"
1. The file system predicts that the process will request next blocks (if reading the first block of a file, you may also need the second, third, etc.)
2. Request the next blocks from the disk
3. As the process requests those blocks, it will already be in cache

Read ahead is great, if blocks are sequentially accessed (next to each other). Not so great if blocks are all over the place, but the file system tries to prevent that when allocating.
