# Operant Conditioning
Also called **instrumental learning**, operant conditioning describes situations in which we can choose among different options based on previous experiences. These options have consequences, and we choose the options that give us the most in return.

> We talk more to people who offer supportive responses, than people who don't.

## Instrumental Processes
Another important figure in operant conditioning is E.L. Thorndike, who studied learning in the 1890s. His best known work is with cats in puzzle bxes:
1. Put cats into a box, which required certain behaviors to open a door
2. Cats tried various methods, called **trial-and-error** learning. Took a long time to pay off and seemed to be random (i.e. meow, scratch at the walls, paw at the bars, push the door with their heads)
3. After repeated trials, cats produce the proper series of behaviors to release the door sooner

![[Pasted image 20220818142012.png]]
> An example of a box used in operant conditioning.

In this example, cats learned to manipulate an instrument such as a petal. As such, Thorndike called this **instrumental learning**.

## Law of Effect
Thorndike was interested in how consequences of behavior influence subsequent behavior.
1. Behaviors that yield satisfying consequences are more likely to recur
2. Behaviors that result in discomfort are less likely to be repeated

When we do things that lead to satisfaction, those responses are more likely to happen in the future. When we engage in behaviors that lead to unpleasant outcomes, we are less likely to do them again.

Thorndike believes that we don't learn the specific consequences of our behavior. Instead, the consequences *stamp in* or *stamp out* the association between the situation and behavior.
* **Satisfaction (stamping in)** - We associate a situation with a behavior when that behavior leads to something pleasant
* **Discomfort (stamping out)** - We do not associate a situation with behavior when that behavior leads to something unpleasant.

Thorndike later removed the stamping out effect when his experiments did not support it.

## Operant Processes
In the 1930s, B.F. Skinner founded radical behaviorism - the philosophy of science that treats thinking and feeling like any other behavior. He differs from Thorndike, in that Skinner included consequence as a part of what we learn. We learn specific stimuli, responses, and outcomes.
* **Antecedents** - Anything in the physical environment that we can detect, and tells us something about the consequences of our actions (i.e. other people, inanimate objects, etc.)
* **Behavior** - Anything we do that is affected by the environment, affects the environment, and can be repeated and counted. 
* **Consequences** - Stimuli that increase or decrease the probability of future behavior. Events that happen after and because of a response

> The *dead man's test* is a test to determine whether something is a behavior. If a dead man can do it, then it isn't a behavior.

## Litt and Schriebman (1981)
A study that tests whether we learn about the antecedents and consequences of behavior.
1. Children with autism are taught how to label different items, being asked to pick up objects by name
2. **Nondifferential consequences** - Children in this group received the same consequence for every correct behavior. Participants took longer to learn these items.
3. **Differentail consequences** - Children in this group received different consequences for each correct behavior. Participants learned these items in fewer trials.

In the experiment:
* Experimenter's prompt is the antecedent
* Child's response is the behavior
* Chip or cookie is the consequence

![[Pasted image 20220818151629.png]]

## Reinforcement Contingencies
Skinner identified four **contingencies** (if, then relationships) that cause a behavior to be more likely or less likely to occur in the future.

In addition, behaviors can be:
* **Positive** - Applies stimulus (pinching, punching, beating, etc.)
* **Negative** - Removes stimulus (phone getting taken away, put in a corner, etc.)

The consequence of a behavior can affect the likelihood of it occuring in the future
* **Reinforcement** - Increases the probability of the behavior occuring again
* **Punishment** - Decreases the probability of the behavior occuring again

The types of stimulus that occur can affect our responses:
* **Appetitive stimulus** - A stimulus that you will work to obtain, also known as a reward (e.g. treat, passing grade, video game, candy)
* **Noxious or Aversive stimulus** - A stimulus you work to avoid (e.g. loud sound, spanking, bad grade, curse word)

![[Pasted image 20220818163646.png]]

## Negative Reinforcement
Negative responses can occur in two forms, both of which is more likely to occur in the future. 
* **Escape** - occurs when the aversive stimulus is already present and a response removes or stops the unpleasant stimulus. Examples include:
	* Going to the docter when sick to get antibiotics, killing bacteria
	* We set an alarm (aversive stimulus) to wake up in the morning. To stop and escape the sound, we press a button
* **Avoidance** occurs when the aversive stimulus is not present, but will occur unless a response is made to cancel the unpleasant event. We experience the escape situation first, before we make an avoidance response.
	* I go to my physician when I am sick so the doctor's treatment will remove the illness—escape conditioning. I also go to my physician twice a year for checkups to detect problems early before they become unpleasant—avoidance conditioning. 
	* We may get a flu shot to avoid contracting an illness. 

![[Pasted image 20220818165401.png]]

## Positive Punishment
Skinner preferred positive reinforcement, since the behavior would still likely occur even if we forget to positively reinforce the behavior. It has a longer-lasting effect on behavior. It also causes a behavior to decrease without aversive events or unpleasant emotions.

On the other hand, if we forget to positively punish an inappropriate response, it would likely occur again. Unless as a last resort, we do not advocate for punishment because of the following reasons:

![[Pasted image 20220818170435.png]]

## Operant Extinction
A consequence previously followed behavior, but now no longer does. Leads to extinction, where responsing is less likely to occur in the future without the consequence. 
1. Behavior is reinforced with a response
2. Reponse suddenly stops after behavior occurs
3. Behavior temporarily increases, called an extinction burst
4. Emotional and aggressive responding
5. Behavior decreases gradually with time
6. Responding eventually stops

![[Pasted image 20220818172305.png]]

Responses can be reinforced every single time they occur (continuous), or occasionally (intermittent).

![[Pasted image 20220818172440.png]]

## Jenkins (1962)
This study tries to determine how different schedules of reinforcement affect responding in extinction:
* Different groups of pigeons received food after key pecks at different frequencies
* Continuous - Pigeons received food after every key peck
* Intermittent - Pigeons received food after half of their key pecks

Pigeons in the continuous group extinguished their behavior more rapidly than the intermittent group, called the **partial reinforcement extinction effect**.

## Shaping
Selecting and reinforcing more complex responses that look like the response you want while extinguishing simpler forms of the target response.

[Youtube link](https://youtu.be/4TyYX5C8uuI)

1. The rat explores the chamber so he can get used to it without startling at every noise
2. Given a food pellet when approaching the lever. Then, notice the rat sniffed the lever
3. Change the requirement for reinforcement to sniffing the lever; sniffing looks more like lever pressing. At this point, approaching the lever without sniffing was put on extinction. We can't demand (e.g., only reinforce) behavior from the rat that we haven't yet seen. We can only put the previous step on extinction once we've seen the next form of behavior.
2. Reinforce touching the lever, once it occurs, and stop reinforcing sniffing.
3. Reinforce pressing the lever, once it occurs, and stop reinforcing touching.

## Reinforcers
Events or stimuli following behavior that increase the likelihood of that kind of response. A reinforcer test is needed to test whether the behavior is a reinforcer or punisher.
1. Measure a baseline frequency for the behavior
2. Apply the appropriate response
3. Measure the new frequency of the behavior

If the behavior is increased, it is a *reinforcer*. On the other hand, if the behavior decreased, it is a *punisher*.

![[Pasted image 20220818181514.png]]

Reinforcers are subdivided into two groups:
* **Primary** - Not learned, including stimuli/events needed to maintain life, such as heat or pain
* **Secondary** - Influences responses because they signal or are associated with a primary reinforcer, such as red for hot, blue for cold.

Secondary reinforcers are subjective, and may change over time. For example, if you have already eaten a lot of candy, the next candy would have negative marginal value.

**Generalized conditioned reinforcers** are objects traded for other reinforcers. Money is a good example, since they don't lose their power to reinforce behavior, and are tied to other primary and secondary reinforcers.

## Schedules of Reinforcement
Rules we use to determine when we get reinforcers for behavior. Discovered when Skinner didn't have enough food for his rats one day. He assumed that rats would stop responding, but they continued to respond in an orderly manner.

![[Pasted image 20220818183724.png]]

Different types of reinforcement schedules can result in different rates of responses.

![[Pasted image 20220818183903.png]]
> A line indicates a delivery of reinforcement

### Fixed Ratio Schedule
Reinforcement is delivered after a *specific number of responses*. The animal learns that a specific number of responses are required to receive reinforcement.

This schedule is characterized by a break-and-run pattern.
1. A run occurs, producing many responses quickly until reward is earned
2. A break occurs after the reinforcer is delivered

> People paid by the number of manufactured items will also take breaks after a specific number of items (5, 10, 15, 20, etc.)

### Variable Ratio Schedule
Instead of delivering the reinforcement after a fixed number of responses, we can deliver after a variable number of responses, around an average. 

Characterized by a high (fast) and constant pattern of responding, since faster responding produces more reinforcers in less time. Unpredictability leads to a constant response, with no pause.

### Fixed Interval Schedule
Response starts a timer, and after a specific amount of time, the next response triggers the delivery of the reinforcer. 

Characterized by a scallop pattern of responding.
1. Respond little at the beginning of the interval
2. Respond increasingly faster toward the end of the interval. 
3. Once reinforcer is delivered, we take a break

> Classes with scheduled quizzes every Friday lead to less studying Monday through Wednesday, and increases studying on the few days before the quiz

### Variable Interval Schedule
Here, we have to wait different amounts of time for each reinforcer. We have to respond once to start the timer, and again to produce a reinforcer after the time ends. Responses can occur before the timer ends, but no reinforcement will occur.

Characterized by a slow and constant pattern of responding. Unpredictability of the reinforcement means we need to keep responding without pausing to maximize the number of reinforcers earned.
